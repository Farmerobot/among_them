# USD per 10^6 tokens

# OPENAI
# https://openai.com/api/pricing/
["openai/gpt-4o"] # 128k context
input_tokens = 2.5
cache_read = 1.25
output_tokens = 10

["openai/gpt-4o-mini"] # 128k context
input_tokens = 0.15
cache_read = 0.075
output_tokens = 0.6

# ANTHROPIC
# https://www.anthropic.com/pricing#anthropic-api
["anthropic/claude-3.5-sonnet"] # 200k context
input_tokens = 3
cache_read = 0
output_tokens = 15

["anthropic/claude-3-5-haiku"] # 200k context
input_tokens = 1
cache_read = 0
output_tokens = 5

# GOOGLE
# https://ai.google.dev/pricing
["google/gemini-flash-1.5"] # 1M context
input_tokens = 0.075
cache_read = 0
output_tokens = 0.3

["google/gemini-pro-1.5"] # 2M context
input_tokens = 1.25
cache_read = 0
output_tokens = 5

# META
# Depends on the provider
["meta-llama/llama-3.2-1b-instruct"] # 131k context
input_tokens = 0.01
cache_read = 0
output_tokens = 0.02

["meta-llama/llama-3.2-3b-instruct"] # 131k context
input_tokens = 0.03
cache_read = 0
output_tokens = 0.05

["meta-llama/llama-3.1-8b-instruct"] # 16k-131k context
input_tokens = 0.05
cache_read = 0
output_tokens = 0.05

["meta-llama/llama-3.1-70b-instruct"] # 33k-131k context
input_tokens = 0.34
cache_read = 0
output_tokens = 0.39

["meta-llama/llama-3.1-405b-instruct"] # 33k-131k context
input_tokens = 1.79
cache_read = 0
output_tokens = 1.79

# MICROSOFT
["microsoft/wizardlm-2-7b"]  # 32k context
input_tokens = 0.055
cache_read = 0
output_tokens = 0.055

["microsoft/wizardlm-2-8x22b"]  # 66k context
input_tokens = 0.5
cache_read = 0
output_tokens = 0.5

# MISTRAL
["mistralai/mistral-nemo"]  # 128k context
input_tokens = 0.15
cache_read = 0
output_tokens = 0.15

# EVA-UNIT-01
["eva-unit-01/eva-qwen-2.5-32b"] # 32k context
input_tokens = 0.5
cache_read = 0
output_tokens = 0.5

# NOUSRESEARCH
["nousresearch/hermes-3-llama-3.1-405b"]
input_tokens = 1.79
cache_read = 0
output_tokens = 2.49

# GRYPHE
["gryphe/mythomax-l2-13b"]  # 4k context (too small)
input_tokens = 0.9
cache_read = 0
output_tokens = 0.9
