{"model_techniques": {"llama-3-1-405b-instruct": {"shifting the burden of proof": 427, "appeal to logic": 774, "bandwagon effect": 196, "projection": 84, "gaslighting": 1, "strategic voting suggestion": 142, "appeal to emotion": 26, "appeal to credibility": 122, "appeal to urgency": 102, "sarcasm": 1, "distraction": 98, "lying": 42, "deception": 17, "appeal to relationship": 12, "feigning ignorance": 5, "appeal to rules": 14, "minimization": 16, "vagueness": 14, "confirmation bias exploitation": 4, "information overload": 2}, "claude-3-5-haiku": {"appeal to credibility": 275, "bandwagon effect": 116, "appeal to logic": 701, "projection": 178, "appeal to emotion": 114, "shifting the burden of proof": 136, "strategic voting suggestion": 319, "gaslighting": 32, "appeal to urgency": 169, "vagueness": 6, "appeal to rules": 10, "confirmation bias exploitation": 5, "lying": 124, "deception": 106, "distraction": 63, "exaggeration": 2, "denial without evidence": 1, "minimization": 3, "appeal to relationship": 4}, "gemini-pro-1-5": {"shifting the burden of proof": 389, "appeal to logic": 646, "appeal to credibility": 175, "strategic voting suggestion": 177, "appeal to urgency": 99, "bandwagon effect": 46, "distraction": 71, "projection": 88, "lying": 105, "deception": 90, "vagueness": 8, "confirmation bias exploitation": 42, "gaslighting": 107, "minimization": 6, "appeal to emotion": 11, "appeal to rules": 12, "denial without evidence": 5, "exaggeration": 1, "feigning ignorance": 2, "appeal to relationship": 5}, "gpt-4o-mini": {"appeal to logic": 740, "appeal to urgency": 224, "shifting the burden of proof": 489, "appeal to credibility": 144, "bandwagon effect": 126, "appeal to emotion": 54, "projection": 118, "appeal to rules": 15, "distraction": 149, "strategic voting suggestion": 189, "minimization": 3, "deception": 31, "lying": 18, "appeal to relationship": 3, "gaslighting": 4, "vagueness": 2}, "llama-3-1-8b-instruct": {"appeal to logic": 726, "bandwagon effect": 105, "distraction": 115, "appeal to urgency": 95, "strategic voting suggestion": 176, "appeal to credibility": 97, "shifting the burden of proof": 479, "projection": 36, "appeal to rules": 58, "vagueness": 9, "appeal to emotion": 38, "appeal to relationship": 12, "confirmation bias exploitation": 1, "lying": 27, "deception": 11, "self-deprecation": 1, "minimization": 6, "information overload": 3, "feigning ignorance": 2}, "gemini-flash-1-5": {"appeal to credibility": 150, "bandwagon effect": 115, "appeal to logic": 671, "distraction": 82, "shifting the burden of proof": 389, "appeal to rules": 45, "strategic voting suggestion": 104, "projection": 16, "appeal to urgency": 86, "appeal to emotion": 4, "lying": 48, "vagueness": 13, "deception": 28, "confirmation bias exploitation": 1, "minimization": 5, "feigning ignorance": 4, "gaslighting": 4, "withholding information": 4, "appeal to relationship": 1}, "claude-3-5-sonnet": {"appeal to credibility": 523, "appeal to logic": 962, "appeal to rules": 36, "bandwagon effect": 67, "distraction": 59, "appeal to emotion": 15, "projection": 210, "shifting the burden of proof": 128, "appeal to urgency": 95, "strategic voting suggestion": 250, "lying": 92, "confirmation bias exploitation": 27, "deception": 70, "gaslighting": 68, "appeal to relationship": 13, "vagueness": 4, "minimization": 14, "sarcasm": 1, "denial without evidence": 3, "summary": 1, "feigning ignorance": 1, "withholding information": 3, "exaggeration": 3}, "gpt-4o": {"appeal to logic": 637, "distraction": 147, "appeal to credibility": 191, "shifting the burden of proof": 552, "projection": 36, "appeal to urgency": 35, "strategic voting suggestion": 84, "bandwagon effect": 79, "appeal to emotion": 34, "appeal to rules": 43, "feigning ignorance": 17, "appeal to relationship": 5, "deception": 56, "lying": 63, "gaslighting": 9, "vagueness": 27, "minimization": 10, "withholding information": 5, "confirmation bias exploitation": 1, "denial without evidence": 1}}, "model_player_counts": {"llama-3-1-405b-instruct": 2099, "claude-3-5-haiku": 2364, "gemini-pro-1-5": 2085, "gpt-4o-mini": 2309, "llama-3-1-8b-instruct": 1997, "gemini-flash-1-5": 1770, "claude-3-5-sonnet": 2645, "gpt-4o": 2032}, "model_input_tokens": {}, "model_output_tokens": {}}