{"model_techniques": {"llama-3-1-405b-instruct": {"appeal to logic": 840, "shifting the burden of proof": 377, "bandwagon effect": 71, "projection": 88, "strategic voting suggestion": 155, "appeal to emotion": 23, "confirmation bias exploitation": 19, "sarcasm": 1, "appeal to credibility": 72, "distraction": 67, "deception": 11, "appeal to relationship": 16, "gaslighting": 4, "appeal to rules": 4, "appeal to urgency": 10, "minimization": 10, "vagueness": 20, "denial without evidence": 1, "information overload": 2, "feigning ignorance": 2, "appeal to fairness": 1}, "claude-3-5-haiku": {"appeal to credibility": 247, "appeal to logic": 737, "projection": 193, "shifting the burden of proof": 105, "strategic voting suggestion": 334, "appeal to urgency": 23, "confirmation bias exploitation": 14, "bandwagon effect": 50, "vagueness": 2, "appeal to rules": 2, "distraction": 46, "denial without evidence": 1, "appeal to emotion": 35, "lying": 1, "deception": 26, "exaggeration": 1, "gaslighting": 3, "minimization": 3, "appeal to relationship": 2, "information overload": 1}, "gemini-pro-1-5": {"appeal to logic": 732, "shifting the burden of proof": 325, "strategic voting suggestion": 185, "bandwagon effect": 25, "projection": 130, "denial without evidence": 5, "vagueness": 12, "confirmation bias exploitation": 12, "minimization": 3, "gaslighting": 22, "appeal to credibility": 121, "deception": 51, "lying": 21, "distraction": 38, "appeal to emotion": 13, "appeal to urgency": 11, "appeal to relationship": 2, "appeal to rules": 3, "withholding information": 1}, "gpt-4o-mini": {"appeal to logic": 795, "projection": 163, "appeal to urgency": 50, "shifting the burden of proof": 374, "strategic voting suggestion": 210, "appeal to credibility": 121, "distraction": 67, "bandwagon effect": 35, "vagueness": 6, "confirmation bias exploitation": 21, "deception": 3, "appeal to emotion": 59, "appeal to relationship": 2, "appeal to rules": 2, "gaslighting": 2}, "llama-3-1-8b-instruct": {"appeal to logic": 764, "strategic voting suggestion": 201, "distraction": 77, "appeal to credibility": 80, "projection": 75, "shifting the burden of proof": 326, "appeal to emotion": 16, "appeal to rules": 11, "bandwagon effect": 16, "appeal to relationship": 19, "vagueness": 9, "appeal to urgency": 19, "denial without evidence": 5, "deception": 5, "self-deprecation": 1, "feigning ignorance": 1, "information overload": 1}, "gemini-flash-1-5": {"appeal to credibility": 157, "appeal to logic": 777, "shifting the burden of proof": 172, "distraction": 83, "bandwagon effect": 34, "strategic voting suggestion": 77, "appeal to urgency": 23, "vagueness": 14, "appeal to rules": 14, "confirmation bias exploitation": 5, "minimization": 3, "deception": 9, "gaslighting": 2, "projection": 15, "denial without evidence": 1, "feigning ignorance": 2, "withholding information": 3, "appeal to emotion": 1}, "claude-3-5-sonnet": {"appeal to logic": 987, "appeal to credibility": 415, "bandwagon effect": 27, "distraction": 45, "projection": 205, "strategic voting suggestion": 273, "shifting the burden of proof": 79, "deception": 27, "appeal to urgency": 18, "confirmation bias exploitation": 31, "withholding information": 3, "gaslighting": 10, "appeal to relationship": 9, "appeal to emotion": 9, "minimization": 5, "lying": 9, "exaggeration": 1, "vagueness": 4, "appeal to rules": 11, "feigning ignorance": 3, "denial without evidence": 2}, "gpt-4o": {"appeal to logic": 797, "bandwagon effect": 28, "shifting the burden of proof": 410, "projection": 41, "strategic voting suggestion": 75, "distraction": 92, "appeal to credibility": 117, "appeal to urgency": 7, "confirmation bias exploitation": 1, "vagueness": 27, "appeal to rules": 5, "appeal to emotion": 7, "appeal to relationship": 5, "denial without evidence": 3, "deception": 8, "feigning ignorance": 6, "minimization": 3, "withholding information": 1}}, "model_player_counts": {"llama-3-1-405b-instruct": 1794, "claude-3-5-haiku": 1826, "gemini-pro-1-5": 1712, "gpt-4o-mini": 1910, "llama-3-1-8b-instruct": 1626, "gemini-flash-1-5": 1392, "claude-3-5-sonnet": 2173, "gpt-4o": 1633}, "model_input_tokens": {}, "model_output_tokens": {}}