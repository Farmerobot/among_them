# Development history

We were assigned to analyse persuasion capabilities of large language models (LLMs) in a game setting. The project was based on the [mk-ai-agents](https://github.com/MarcinKorcz101/mk-ai-agents) project, which was developed by Marcin Korcz. The original project has been heavily modified and extended to meet the requirements of our analysis.
Right when we started working on this project we knew we need to adjust existing code to our needs. The original project was a terminal application. This introduced a few limitations:
- It was hard to track the game state and debug the game
- there was no way to pause the game for partial analysis and resume it later
- the game state was not saved, so there is no way to reproduce the discussion based on the same previous actions
- there was no cost estimation - we did not know how much a full game costs
Considering all of this we decided to create a gui for easier analysis of llm responses and actions taken by ai agents. We first started experimenting with tkinter and started using pydantic library to clean up the code a bit and to make sure the classes are properly defined and all class variables are clearly visible in code. and not hidden in some functions that call self.variable=something. We added some new classes with abstract methods. We also rewrote all the prompts to make them shorter (got rid of unnecessary context) and longer (append game description at the beginning and also describe better output format for the ai to follow, because often in original project llms did not conform to the output format). To understand code better we tried to write some unit tests. We created a github workflow to run tests on every push to the repository. We updated the game engine to use new classes and we tried to add some debugging print statements and standardise how the gamehistory was saved. We created a new class to track game state and player state and history of states for each round. This included llm responses and prompts, which were not visible in original repository in any way. the new state and history classes allowed us to easily track and debug llm responses and prompts over time. After that we concluded tkinter is not the best choice for our project and we switched to streamlit. We also added support for gemini llm models. we separated large file for game models into smaller files. At first we displayer raw game state json which included all the information about the game. Then we added some columns with information about each player (with icons of role, current status (alive, eliminated) and progress for tasks completed). At first we did not how streamlit works and a lot of information was doubled when we tried to update streamlit text fields. Later we discovered that each call to streamlit element adds a new element and each click of button causes the entire script to rerun. In the original project there was only one function main_game_loop to call and this will run entire game without stops. This was a problem. After learning this we concluded we need to rewrite game engine to perform single steps at a time. We wanted to:
- save the game state after each step
- display game state in streamlit nicely - read only.
- rerun the streamlit script as much as we want because it is only reading the game state from json file and displaying it.
- After clicking "make step" button the game will perform one step and save the new state to json file. then we call st.rerun() to display new game state. 
To not waste our money on testing the gui we temporarily created fake ai agent that responds with template responses. just for testing. We concluded that sidebar would be better than columns for displaying the players state, because we wanted to display details in main panel. We updated the sidebar player info to include current location, seen actions, and task list as well as impostor cooldown. We completed the game engine rewrite to perform single steps and save the game state after each step to game_state.json file. this file could be copied and saved for later analysis. At this moment the game engine is almost entirely rewritten as well as other files. There is action step, discussion step and voting, current player index, round number and everything needed to resume the game state after loading the game. We also improved logging to make it more readable and changed the way player observations are stored and made them readable - previously they were raw Enums for example "GameLocation.LOC_CAFETERIA" instead of "Cafeteria".
After all that we started working on actual thing we were supposed to do - analysis of persuasion capabilities. We researched the web for some papers bout persuasion techniques and created a prompt for llm to analyse and annotate text with persuasion techniques from these papers. We also tried with counting the number of times each technique was used by a player. We did not use structured output feature for this. We just told llm to stick to output format. We separated game engine, gui handler and analyser classes. Then we added simple cost summary. we counted token usage for each player and total cost usage and multiplied it by cost and disply it in raw json. We Refactored project structure for better organization and maintainability. In the main panel we added a map (plotly with background and custom location points calculated by hand) and we displayed player locations on map based on gameLocation placement on the graph. We added some noise so the players are not in the same place. next to map we display game logs. At this time we switched llm provider to openrouter. this seemed to be the best option for us because with one key we have access to many large language models from different companies for the lowest price. We added current player indicator and added support for keys saved in .env files and not just exported before running program. We added some script to run a gui with simple command. We fixed some problems we had after rewrite - tasks were not displayed correctly and current player state was not displayed correctly. We also got rid of ascii map as it introduced hallucinaions for llms to go to places they can not to. Now we added "Make step" button. We just skipped this part as refreshing the website was making step. Now refreshing does nothing and we need to press this new button to make a step. We added a button to analyze discussion and display annotated version of this discussion with annotations about persuasion techniques. We made the page "wide" by default to fit more on the screen and implemented player selection. Now the discussion can be analysed for specific player (llm model). We also added nice discussion log under the map. We created a cost estimation graph. Polynomial regression is used to estimate game cost for each player (llm model) and total cost based on data from previous rounds. The estimation is for next 5 rounds. Under the cost estimation graph we also display chat like history of llm prompts and responses for easier debugging and to see llm thought process (discussion points, action plan) which affects the actions taken by llm. We fixed some tests and we decided to throw an error when llm does not conform to output format. This allows to make the step again. in original project when this occured the "wait" action was taken. We decided to run the game with 5 different "Roleplay" model available via openrouter. They were the top used ones on openrouter site. We noticed that roleplay models excel at discussion. The discussion by "roleplay" models was far more natural and models were more persuasive.
