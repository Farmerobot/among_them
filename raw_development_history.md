# Development history

We were assigned to analyse persuasion capabilities of large language models (LLMs) in a game setting. The project was based on the [mk-ai-agents](https://github.com/MarcinKorcz101/mk-ai-agents) project, which was developed by Marcin Korcz. The original project has been heavily modified and extended to meet the requirements of our analysis.
Right when we started working on this project we knew we need to adjust existing code to our needs. The original project was a terminal application. This introduced a few limitations:
- It was hard to track the game state and debug the game
- there was no way to pause the game for partial analysis and resume it later
- the game state was not saved, so there is no way to reproduce the discussion based on the same previous actions
- there was no cost estimation - we did not know how much a full game costs
Considering all of this we decided to create a gui for easier analysis of llm responses and actions taken by ai agents. We first started experimenting with tkinter and started using pydantic library to clean up the code a bit and to make sure the classes are properly defined and all class variables are clearly visible in code. and not hidden in some functions that call self.variable=something. We added some new classes with abstract methods. We also rewrote all the prompts to make them shorter (got rid of unnecessary context) and longer (append game description at the beginning and also describe better output format for the ai to follow, because often in original project llms did not conform to the output format). To understand code better we tried to write some unit tests. We created a github workflow to run tests on every push to the repository. We updated the game engine to use new classes and we tried to add some debugging print statements and standardise how the gamehistory was saved. We created a new class to track game state and player state and history of states for each round. This included llm responses and prompts, which were not visible in original repository in any way. the new state and history classes allowed us to easily track and debug llm responses and prompts over time. After that we concluded tkinter is not the best choice for our project and we switched to streamlit. We also added support for gemini llm models. we separated large file for game models into smaller files. At first we displayer raw game state json which included all the information about the game. Then we added some columns with information about each player (with icons of role, current status (alive, eliminated) and progress for tasks completed). At first we did not how streamlit works and a lot of information was doubled when we tried to update streamlit text fields. Later we discovered that each call to streamlit element adds a new element and each click of button causes the entire script to rerun. In the original project there was only one function main_game_loop to call and this will run entire game without stops. This was a problem. After learning this we concluded we need to rewrite game engine to perform single steps at a time. We wanted to:
- save the game state after each step
- display game state in streamlit nicely - read only.
- rerun the streamlit script as much as we want because it is only reading the game state from json file and displaying it.
- After clicking "make step" button the game will perform one step and save the new state to json file. then we call st.rerun() to display new game state. 
To not waste our money on testing the gui we temporarily created fake ai agent that responds with template responses. just for testing. We concluded that sidebar would be better than columns for displaying the players state, because we wanted to display details in main panel. We updated the sidebar player info to include current location, seen actions, and task list as well as impostor cooldown. We completed the game engine rewrite to perform single steps and save the game state after each step to game_state.json file. this file could be copied and saved for later analysis. At this moment the game engine is almost entirely rewritten as well as other files. There is action step, discussion step and voting, current player index, round number and everything needed to resume the game state after loading the game. We also improved logging to make it more readable and changed the way player observations are stored and made them readable - previously they were raw Enums for example "GameLocation.LOC_CAFETERIA" instead of "Cafeteria".
After all that we started working on actual thing we were supposed to do - analysis of persuasion capabilities. We researched the web for some papers bout persuasion techniques and created a prompt for llm to analyse and annotate text with persuasion techniques from these papers. We also tried with counting the number of times each technique was used by a player. We did not use structured output feature for this. We just told llm to stick to output format. We separated game engine, gui handler and analyser classes. Then we added simple cost summary. we counted token usage for each player and total cost usage and multiplied it by cost and disply it in raw json. We Refactored project structure for better organization and maintainability. In the main panel we added a map (plotly with background and custom location points calculated by hand) and we displayed player locations on map based on gameLocation placement on the graph. We added some noise so the players are not in the same place. next to map we display game logs. At this time we switched llm provider to openrouter. this seemed to be the best option for us because with one key we have access to many large language models from different companies for the lowest price. We added current player indicator and added support for keys saved in .env files and not just exported before running program. We added some script to run a gui with simple command. We fixed some problems we had after rewrite - tasks were not displayed correctly and current player state was not displayed correctly. We also got rid of ascii map as it introduced hallucinaions for llms to go to places they can not to. Now we added "Make step" button. We just skipped this part as refreshing the website was making step. Now refreshing does nothing and we need to press this new button to make a step. We added a button to analyze discussion and display annotated version of this discussion with annotations about persuasion techniques. We made the page "wide" by default to fit more on the screen and implemented player selection. Now the discussion can be analysed for specific player (llm model). We also added nice discussion log under the map. We created a cost estimation graph. Polynomial regression is used to estimate game cost for each player (llm model) and total cost based on data from previous rounds. The estimation is for next 5 rounds. Under the cost estimation graph we also display chat like history of llm prompts and responses for easier debugging and to see llm thought process (discussion points, action plan) which affects the actions taken by llm. We fixed some tests and we decided to throw an error when llm does not conform to output format. This allows to make the step again. in original project when this occured the "wait" action was taken. We decided to run the game with 5 different "Roleplay" model available via openrouter. They were the top used ones on openrouter site. We noticed that roleplay models excel at discussion. The discussion by "roleplay" models was more natural and models were more persuasive.
In our annotation visualization we made the change to display multiple techniques in single annotation and we fixed formatting to display messages in new line. we also generated a summary of techniques used by player (model) for impostor and crewmate teams (model comparison). We tried to improve prompts. Now all player history is in prompt for each round and also previous plans. This makes the prompt heavy in input tokens but it works perfeclty with propmt caching. We fixed a bug - when there is a tie (nobody-2 Alice-2) noone was ejected beause of tie, but Alice should be ejected - it is fixed now. We added tounament persuasion techniques analysis tab where all the previous game states located in data/tournaments are analyzed and all the persuasion techniques used by all models are listed. The tournaments are analyzed simultaniously with gpt-4o-mini model with multithreading. In GUI we added some useful buttons to: 
- clear game state (no more copy pasting game_state.json files), 
- make steps infinitiely (no more clicking make step button), 
- save current game state to tournaments folder,
- force setting action, discussion and voting phase to test the game in specific phase
We changed temperature everywhere from 0.1 to: 0 for action and vote selection, 1 for planning and discussion points and 0.5 for discussion message. Prompts are now few-shot. We also adjusted the code to use system and user prompts. This helped A LOT. smaller models thanks to this change produced better results. There were no longer that many llm errors like incorrect formatting and discussion felt more natural. We cleaned up code for agents classes. We fixed an error where sidebar was not updated. 
We added a game settings page that shows up when no game state is currently loaded. This page includes setting number of crewmates and impostors and selecting a model for each team. All the game settings including prompts can be changed here and new game can be started with selected settings and prompts. We fixed the llm responses visualisation to include current state. We added a new tab with persuasion techniques listed. We added "Utilize persuasion techniques helping achieve your objectives." to prompts for llms to make them use more persuasion techniques but we did not list them. The models by themselves were using persuasion techniques but we plan to analyze what techniques do they use and how many when they are asked to use them. We wrote a script to run tournaments. Now we can specify list of models, number of impostors/crewmates, number of repetitions and the script will run the tournament each model with each. The players are shuffled and names are chosen. For each game we track what models were used, who won and whether there was an exception other than llm not conforming to output format. Then, in other script all games are analyzed to produce a table with "Impostor", "Crewmate", "Wins", "Loses", "Exceptions", "Round limits", "Average game price" and results are saved in csv file. We researched and updated persuasion techniques to include a list of 28 techniques. We implemented better action capturing from llm responses. Now response "Alice decided to: move to location Cafeteria" is correctly captured as action "move to location Cafeteria". We made the logs cleaner and adjusted gui to work with tournaments. Now when tournament is run in separate terminal, you can in other terminal "poetry run main" and the gui will show the game state and update it when it changes during tournament. Games are automatically switched so for each game in tournament you can see the game state as it is. Enen though all "kill" words are replaced with "eliminate" some llms still do not want to "provide information or guidance on harmful behaviour" thus action is not selected. 